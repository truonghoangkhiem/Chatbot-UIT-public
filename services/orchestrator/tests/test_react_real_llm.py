#!/usr/bin/env python3
"""
Test ReAct Framework v·ªõi LLM th·∫≠t (OpenRouter) - S·ª≠ d·ª•ng c·∫•u h√¨nh h·ªá th·ªëng

Ch·∫°y: cd services/orchestrator && python tests/test_react_real_llm.py
"""

import asyncio
import sys
import os
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any
from datetime import datetime

# Setup paths BEFORE any imports
orchestrator_path = Path(__file__).parent.parent.resolve()
rag_services_path = orchestrator_path.parent / "rag_services"

# IMPORTANT: Insert orchestrator FIRST so its 'app' package takes priority
# over rag_services/app
sys.path.insert(0, str(rag_services_path))  # Second priority
sys.path.insert(0, str(orchestrator_path))  # First priority (will be checked first)

# Load environment
from dotenv import load_dotenv
env_path = orchestrator_path / ".env"
if not env_path.exists():
    env_path = rag_services_path / ".env"
load_dotenv(env_path)

# Import from packages now that path is set
from app.adapters.openrouter_adapter import OpenRouterAdapter
from app.core.domain import (
    AgentRequest, ConversationContext, ConversationMessage, 
    ConversationRole, MessageType
)
from app.agents.graph_reasoning_agent import GraphReasoningAgent, GraphQueryType
from adapters.graph.neo4j_adapter import Neo4jGraphAdapter


def print_separator(title):
    """Print section separator."""
    print(f"\n{'='*70}")
    print(f"üß™ {title}")
    print(f"{'='*70}")


def print_header():
    """Print test suite header."""
    print("ü§ñ"*35)
    print("    REACT + REAL LLM TEST SUITE")
    print("    Using: OpenRouterAdapter from Orchestrator")
    print("ü§ñ"*35)


# ========== LLM PORT WRAPPER ==========

class LLMPortWrapper:
    """
    Wrapper ƒë·ªÉ adapt OpenRouterAdapter interface cho GraphReasoningAgent.
    
    GraphReasoningAgent g·ªçi: llm_port.generate(messages=..., temperature=..., max_tokens=...)
    OpenRouterAdapter c·∫ßn: generate_response(AgentRequest)
    """
    
    def __init__(self, openrouter_adapter):
        """Initialize wrapper with OpenRouterAdapter."""
        self.adapter = openrouter_adapter
        self.call_count = 0
    
    async def generate(
        self, 
        messages: List[Dict[str, str]], 
        temperature: float = 0.3, 
        max_tokens: int = 500
    ):
        """
        Generate response - adapts to GraphReasoningAgent's expected interface.
        """
        self.call_count += 1
        
        # Extract system prompt and conversation messages
        system_prompt = None
        conv_messages = []
        user_prompt = ""
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                system_prompt = content
            elif role == "user":
                user_prompt = content  # Last user message becomes prompt
                conv_messages.append(ConversationMessage(
                    role=ConversationRole.USER,
                    content=content,
                    timestamp=datetime.now(),
                    message_type=MessageType.TEXT
                ))
            elif role == "assistant":
                conv_messages.append(ConversationMessage(
                    role=ConversationRole.ASSISTANT,
                    content=content,
                    timestamp=datetime.now(),
                    message_type=MessageType.TEXT
                ))
        
        # Create context
        context = ConversationContext(
            session_id="react-test",
            messages=conv_messages[:-1] if conv_messages else [],
            system_prompt=system_prompt,
            temperature=temperature
        )
        
        # Create request
        request = AgentRequest(
            prompt=user_prompt,
            context=context,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=False
        )
        
        print(f"   üì§ LLM call #{self.call_count}")
        
        # Call adapter
        response = await self.adapter.generate_response(request)
        
        print(f"      Tokens: {response.tokens_used}, Time: {response.processing_time:.2f}s")
        
        return response  # AgentResponse has .content field


# ========== TEST FUNCTIONS ==========

async def test_llm_connection():
    """Test LLM connection using OpenRouterAdapter."""
    print_separator("TEST 1: LLM Connection (OpenRouterAdapter)")
    
    api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå No API key found (OPENROUTER_API_KEY or OPENAI_API_KEY)")
        return False
    
    print(f"   API Key: {api_key[:20]}...")
    
    adapter = OpenRouterAdapter(
        api_key=api_key,
        default_model="google/gemini-2.0-flash-001",
        timeout=60
    )
    
    # Simple test
    request = AgentRequest(
        prompt="Tr·∫£ l·ªùi ng·∫Øn g·ªçn: 1 + 1 = ?",
        temperature=0.1,
        max_tokens=50
    )
    
    response = await adapter.generate_response(request)
    
    print(f"\nüìù Response: {response.content}")
    print(f"   Model: {response.model_used}")
    print(f"   Tokens: {response.tokens_used}")
    
    await adapter.close()
    
    assert response.content, "Response should not be empty"
    print("\n‚úÖ LLM connection OK")
    return True


async def test_react_with_system_llm():
    """Test ReAct loop v·ªõi LLM t·ª´ h·ªá th·ªëng v√† Neo4j th·∫≠t."""
    print_separator("TEST 2: ReAct Loop v·ªõi System LLM")
    
    # Get API key
    api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå No API key found")
        return False
    
    print("\nüì° Initializing components...")
    
    # Neo4j
    neo4j_adapter = Neo4jGraphAdapter(
        uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        username=os.getenv("NEO4J_USER", "neo4j"),
        password=os.getenv("NEO4J_PASSWORD", "password")
    )
    
    healthy = await neo4j_adapter.health_check()
    if not healthy:
        print("‚ùå Neo4j not available, skipping test")
        return False
    print("   ‚úì Neo4j connected")
    
    # LLM
    openrouter = OpenRouterAdapter(
        api_key=api_key,
        default_model="google/gemini-2.0-flash-001",
        timeout=60
    )
    llm_port = LLMPortWrapper(openrouter)
    print("   ‚úì OpenRouterAdapter connected")
    
    # Create agent
    agent = GraphReasoningAgent(
        graph_adapter=neo4j_adapter,
        llm_port=llm_port
    )
    print("   ‚úì GraphReasoningAgent created with ReAct")
    
    # Test queries
    test_queries = [
        {
            "query": "ƒêi·ªÅu 14 c·ªßa quy ch·∫ø ƒë√†o t·∫°o quy ƒë·ªãnh v·ªÅ v·∫•n ƒë·ªÅ g√¨?",
            "type": GraphQueryType.MULTI_HOP,
        },
    ]
    
    results = []
    
    for i, test in enumerate(test_queries, 1):
        print(f"\n{'‚îÄ'*60}")
        print(f"üìù Query {i}: {test['query']}")
        print(f"{'‚îÄ'*60}")
        
        result = await agent.reason(
            query=test["query"],
            query_type=test["type"],
            context={}
        )
        
        print(f"\nüìä Results:")
        print(f"   Confidence: {result.confidence:.2f}")
        print(f"   Nodes found: {len(result.nodes)}")
        print(f"   Paths found: {len(result.paths)}")
        print(f"   LLM calls: {llm_port.call_count}")
        
        print(f"\nüîÑ Reasoning Chain ({len(result.reasoning_steps)} steps):")
        for j, step in enumerate(result.reasoning_steps, 1):
            step_display = step[:100] + "..." if len(step) > 100 else step
            print(f"   {j}. {step_display}")
        
        if result.nodes:
            print(f"\nüìÑ Sample Nodes:")
            for node in result.nodes[:3]:
                name = node.get("title") or node.get("name") or "Unknown"
                print(f"      ‚Ä¢ {name[:60]}...")
        
        results.append({
            "query": test["query"],
            "success": result.confidence > 0.3,
            "confidence": result.confidence,
            "steps": len(result.reasoning_steps),
            "nodes": len(result.nodes)
        })
    
    # Cleanup
    await openrouter.close()
    
    # Summary
    print(f"\n{'='*60}")
    print("üìä SUMMARY")
    print(f"{'='*60}")
    
    for r in results:
        status = "‚úÖ" if r["success"] else "‚ùå"
        print(f"   {status} {r['query'][:40]}... (conf={r['confidence']:.2f}, nodes={r['nodes']})")
    
    all_passed = all(r["success"] for r in results)
    print(f"\n   Total LLM calls: {llm_port.call_count}")
    print(f"   Result: {'‚úÖ ALL PASSED' if all_passed else '‚ö†Ô∏è SOME FAILED'}")
    
    return all_passed


async def test_react_complex_query():
    """Test ReAct v·ªõi c√¢u h·ªèi ph·ª©c t·∫°p."""
    print_separator("TEST 3: Complex Query")
    
    api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå No API key found")
        return False
    
    # Initialize
    neo4j_adapter = Neo4jGraphAdapter(
        uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        username=os.getenv("NEO4J_USER", "neo4j"),
        password=os.getenv("NEO4J_PASSWORD", "password")
    )
    
    healthy = await neo4j_adapter.health_check()
    if not healthy:
        print("‚ùå Neo4j not available")
        return False
    
    openrouter = OpenRouterAdapter(
        api_key=api_key,
        default_model="google/gemini-2.0-flash-001",
        timeout=60
    )
    llm_port = LLMPortWrapper(openrouter)
    agent = GraphReasoningAgent(graph_adapter=neo4j_adapter, llm_port=llm_port)
    
    # Complex query
    query = "Sinh vi√™n b·ªã bu·ªôc th√¥i h·ªçc trong nh·ªØng tr∆∞·ªùng h·ª£p n√†o?"
    
    print(f"\nüìù Query: {query}")
    
    result = await agent.reason(
        query=query,
        query_type=GraphQueryType.MULTI_HOP,
        context={}
    )
    
    print(f"\nüîÑ Reasoning ({len(result.reasoning_steps)} steps):")
    for i, step in enumerate(result.reasoning_steps, 1):
        print(f"   {i}. {step[:80]}...")
    
    print(f"\n   Confidence: {result.confidence:.2f}")
    print(f"   Nodes: {len(result.nodes)}")
    print(f"   LLM calls: {llm_port.call_count}")
    
    # Cleanup
    await openrouter.close()
    
    print("\n‚úÖ Complex query test completed")
    return result.confidence > 0.3


async def main():
    """Run all tests."""
    print_header()
    
    # Check API key
    api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå No API key found in environment")
        print("   Set OPENROUTER_API_KEY or OPENAI_API_KEY")
        return False
    
    tests = [
        ("LLM Connection", test_llm_connection),
        ("ReAct with System LLM", test_react_with_system_llm),
        ("Complex Query Test", test_react_complex_query),
    ]
    
    results = []
    
    for name, test_func in tests:
        try:
            result = await test_func()
            results.append((name, result, None))
        except Exception as e:
            print(f"\n‚ùå Test failed: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False, str(e)))
    
    # Final summary
    print(f"\n{'='*70}")
    print(f"üß™ FINAL SUMMARY")
    print(f"{'='*70}")
    
    passed = 0
    for name, success, error in results:
        if success:
            print(f"   ‚úÖ PASSED: {name}")
            passed += 1
        else:
            print(f"   ‚ùå FAILED: {name}")
            if error:
                print(f"      Error: {error[:50]}...")
    
    print(f"\n   Total: {passed}/{len(results)} tests passed")
    print(f"{'='*70}")
    
    return passed == len(results)


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
